{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7525f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/itv009538/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81316cdb",
   "metadata": {},
   "source": [
    "### RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f8a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"/public/trendytech/retail_db/orders/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fea7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25 00:00:00.0,11599,CLOSED',\n",
       " '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT',\n",
       " '3,2013-07-25 00:00:00.0,12111,COMPLETE',\n",
       " '4,2013-07-25 00:00:00.0,8827,CLOSED',\n",
       " '5,2013-07-25 00:00:00.0,11318,COMPLETE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d6db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_rdd = orders_rdd.map(lambda x: (int(x.split(\",\")[0]), x.split(\",\")[1], int(x.split(\",\")[2]), x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67f52ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '2013-07-25 00:00:00.0', 11599, 'CLOSED'),\n",
       " (2, '2013-07-25 00:00:00.0', 256, 'PENDING_PAYMENT'),\n",
       " (3, '2013-07-25 00:00:00.0', 12111, 'COMPLETE'),\n",
       " (4, '2013-07-25 00:00:00.0', 8827, 'CLOSED'),\n",
       " (5, '2013-07-25 00:00:00.0', 11318, 'COMPLETE')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_orders_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af992cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = 'order_id long, order_date string, cust_id int, order_status string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4367efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(new_orders_rdd, orders_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15117c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+---------------+\n",
      "|order_id|          order_date|cust_id|   order_status|\n",
      "+--------+--------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:...|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|  12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|   8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|  11318|       COMPLETE|\n",
      "+--------+--------------------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67dde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- cust_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467a672",
   "metadata": {},
   "source": [
    "### External resources\n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
